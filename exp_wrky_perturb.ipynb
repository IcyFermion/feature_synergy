{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from multiprocessing import shared_memory\n",
    "from multiprocessing.dummy import Pool\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import multiprocessing as mp\n",
    "from itertools import chain, combinations\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import mp_run\n",
    "\n",
    "import concurrent.futures\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_factor = 3\n",
    "num_rf_predictors = 500\n",
    "\n",
    "induction_flag = 1\n",
    "mp_threads = 20\n",
    "# if (len(sys.argv)>=3):\n",
    "#     induction_flag = bool(sys.argv[1])\n",
    "#     mp_threads = int(sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_df = pd.read_csv('data/wrky_regulators.csv')\n",
    "tf_list = tf_df['Gene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_2_3(iterable):\n",
    "    \"powerset([1,2,3]) -->  (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.read_csv('data/wrky_targets_neg.csv')\n",
    "if (induction_flag):\n",
    "    target_df = pd.read_csv('data/wrky_targets_pos.csv')\n",
    "deg_genes = target_df['Gene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = pd.read_csv('data/GSE97500/expression.tsv', sep='\\t', index_col=0)\n",
    "c = pd.Series(list(set(deg_genes).intersection(set(ts_df.index))))\n",
    "tf_list = pd.Series(list(set(tf_list).intersection(set(ts_df.index))))\n",
    "meta_df = pd.read_csv('data/GSE97500/meta_data.tsv', sep='\\t')\n",
    "ts_exp_index = meta_df[meta_df['isTs']]\n",
    "ts_exp_index_target =  ts_exp_index[ts_exp_index['is1stLast'] != 'f'].condName\n",
    "ts_exp_index_source =  ts_exp_index[ts_exp_index['is1stLast'] != 'f'].prevCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_genes = set(deg_genes).intersection(set(ts_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_trivial_targets = []\n",
    "for target in target_genes:\n",
    "    if ts_df.loc[target].mean() != 0.0:\n",
    "        non_trivial_targets.append(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_genes = pd.Series(non_trivial_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 12/775 [07:22<7:56:50, 37.50s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regr = RandomForestRegressor(random_state=0)\n",
    "\n",
    "ts_train_y_list = ts_df[ts_exp_index_target]\n",
    "\n",
    "result_list = []\n",
    "result_measure_list = []\n",
    "\n",
    "for target_gene in tqdm(target_genes):\n",
    "    train_gene_index = tf_list[tf_list != target_gene]\n",
    "    ts_train_X = ts_df[ts_exp_index_source].T[train_gene_index]\n",
    "\n",
    "    ts_train_y = ts_train_y_list.loc[target_gene]\n",
    "\n",
    "    input_mean = ts_train_X.mean()\n",
    "    input_std = ts_train_X.std()\n",
    "    perturbation_input = input_mean.copy()\n",
    "    perturbation_input = np.repeat(np.array(input_mean), repeats=len(input_mean), axis=0)\n",
    "    perturbation_input = perturbation_input.reshape(len(input_mean),len(input_mean)) + np.diagflat(np.array([ts_train_X.values.std()]*len(input_mean)) * perturbation_factor)\n",
    "\n",
    "    func = partial(mp_run.rf_feature_importance, ts_train_X, ts_train_y)\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        results = pool.map(func, range(num_rf_predictors))\n",
    "\n",
    "    base_predictions = [result.predict(np.array(input_mean).reshape(1,-1))[0] for result in results]\n",
    "    y_std = ts_train_y.std()\n",
    "\n",
    "    perturbation_input_list = np.array_split(perturbation_input, 10)\n",
    "    perturbation_input_shapes = [p.shape for p in perturbation_input_list]\n",
    "    shared_memory_list = []\n",
    "    for i, p in enumerate(perturbation_input_list):\n",
    "        shm = shared_memory.SharedMemory(create=True, size=p.nbytes, name=\"perturbation_input_\"+str(i))\n",
    "        buffer = np.ndarray(p.shape, dtype=p.dtype, buffer=shm.buf)\n",
    "        buffer[:] = p[:]\n",
    "        shared_memory_list.append(shm)\n",
    "\n",
    "\n",
    "    def f(perturbation_input_shapes, x):\n",
    "        pred_list = []\n",
    "        for i, shape in enumerate(perturbation_input_shapes):\n",
    "\n",
    "            # Attach to the existing shared memory\n",
    "            existing_shm = shared_memory.SharedMemory(name='perturbation_input_'+str(i))\n",
    "            # Read from the shared memory (we know the size is 1)\n",
    "            c = np.ndarray(shape, dtype=np.float64, buffer=existing_shm.buf)\n",
    "            pred_list.append(x.predict(c))\n",
    "            existing_shm.close()\n",
    "        return np.concatenate(pred_list)\n",
    "\n",
    "    func = partial(f, perturbation_input_shapes)\n",
    "\n",
    "    with mp.Pool(processes=mp_threads) as pool:\n",
    "        perturbation_predictions = pool.map(func, results)\n",
    "\n",
    "    for shm in shared_memory_list:\n",
    "        shm.close()\n",
    "        shm.unlink()\n",
    "\n",
    "\n",
    "    perturbation_measures = [(perturbation_prediction - base_prediction)/y_std for perturbation_prediction, base_prediction in zip(perturbation_predictions, base_predictions)]\n",
    "    importance_matrix = np.array(perturbation_measures).T\n",
    "    importance_df = pd.DataFrame(index=train_gene_index, data=importance_matrix, columns=range(num_rf_predictors))\n",
    "    importance_df_list = []\n",
    "    mean_importance = importance_df.mean(axis=1)\n",
    "    if (induction_flag):\n",
    "        top_influence_genes = train_gene_index[np.argsort(mean_importance)[::-1][:5]]\n",
    "    else:\n",
    "        top_influence_genes = train_gene_index[np.argsort(mean_importance)[:5]]\n",
    "    importance_df_list.append(mean_importance)\n",
    "    data_mean = ts_df.T[top_influence_genes].mean()\n",
    "    data_std = ts_df.T[top_influence_genes].std()\n",
    "    regr = RandomForestRegressor(random_state=42, warm_start=True, n_estimators=100, n_jobs=20)\n",
    "    ts_train_X = ts_df[ts_exp_index_source].T[top_influence_genes]\n",
    "    regr = regr.fit(ts_train_X, ts_train_y)\n",
    "\n",
    "    base_prediction = regr.predict(np.array(data_mean).reshape(1,-1))[0]\n",
    "    y_std = ts_df.T.std()[target_gene]\n",
    "    perturbation_list = list(choose_2_3(top_influence_genes))\n",
    "\n",
    "    perturbation_result_list = []\n",
    "    perturbation_list_names = ['; '.join(perturbation_genes) for perturbation_genes in perturbation_list]\n",
    "    for perturbation_genes in perturbation_list:\n",
    "        perturbation_input = data_mean.copy()\n",
    "        for gene in perturbation_genes:\n",
    "            perturbation_input[gene] += data_std[gene] * perturbation_factor\n",
    "        perturbation_prediction = regr.predict(np.array(perturbation_input).reshape(1,-1))[0]\n",
    "        perturbation_measure = (perturbation_prediction - base_prediction)/y_std\n",
    "        perturbation_result_list.append(perturbation_measure)\n",
    "    if (induction_flag):\n",
    "        result_list.append(np.array(perturbation_list_names)[np.argsort(perturbation_result_list)[::-1][:5]])\n",
    "        result_measure_list.append(np.array(perturbation_result_list)[np.argsort(perturbation_result_list)[::-1][:5]])\n",
    "    else:\n",
    "        result_list.append(np.array(perturbation_list_names)[np.argsort(perturbation_result_list)[:5]])\n",
    "        result_measure_list.append(np.array(perturbation_result_list)[np.argsort(perturbation_result_list)[:5]])\n",
    "        \n",
    "    del importance_df_list\n",
    "    del importance_df\n",
    "    del shared_memory_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shm in shared_memory_list:\n",
    "        shm.close()\n",
    "        shm.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_measure_list = np.array(result_measure_list)\n",
    "result_list = np.array(result_list)\n",
    "out_df = pd.DataFrame()\n",
    "out_df.index = target_genes[:50]\n",
    "for i in range(5):\n",
    "    comb_name = 'top_{}_combination'.format(i+1)\n",
    "    score_name = 'top_{}_score'.format(i+1)\n",
    "    out_df[comb_name] = result_list[:,i]\n",
    "    out_df[score_name] = result_measure_list[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (induction_flag):\n",
    "    out_df.to_csv('output/wrky_presentation_comb_pos.csv')\n",
    "else:\n",
    "    out_df.to_csv('output/wrky_presentation_comb_neg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('inf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45e8b4e597901076a3793f077aa4917c88e4588bf4d8f42473ef14bed5440d9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
